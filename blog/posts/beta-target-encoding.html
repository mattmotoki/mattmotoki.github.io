<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Beta Target Encoding</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                packages: ['base', 'ams']  // Add AMS package
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>

    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            font-size: 16px;
        }

        @media (max-width: 600px) {
            body {
                font-size: 14px;
                padding: 10px;
            }
        }

        h1,
        h2,
        h3 {
            color: #2c3e50;
            line-height: 1.2;
        }

        h1 {
            font-size: 2em;
        }

        h2 {
            font-size: 1.5em;
        }

        h3 {
            font-size: 1.2em;
        }

        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
        }

        .caption {
            font-style: italic;
            text-align: center;
            margin-top: 10px;
            font-size: 0.9em;
        }

        pre {
            overflow-x: auto;
            background-color: #f6f8fa;
            padding: 10px;
            border-radius: 5px;
        }

        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            font-size: 0.9em;
            background-color: #f6f8fa;
            padding: 2px 4px;
            border-radius: 3px;
        }

        a {
            color: #3498db;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .mjx-chtml {
            font-size: 100% !important;
            overflow-x: auto;
            overflow-y: hidden;
        }


        .equation-container {
            width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
            margin: 20px 0;
            position: relative;
        }

        .mjx-container {
            overflow-x: auto;
            overflow-y: hidden;
            min-width: 100%;
            padding: 10px 0;
            display: flex;
            justify-content: center;
        }

        .mjx-math {
            padding-right: 3em;
            /* Space for equation number */
            display: inline-block;
        }

        .mjx-eqn-number {
            position: absolute;
            right: 0;
            top: 50%;
            transform: translateY(-50%);
            padding-right: 10px;
        }

        /* Style the scrollbar for webkit browsers */
        .equation-container::-webkit-scrollbar {
            height: 8px;
        }

        .equation-container::-webkit-scrollbar-thumb {
            background-color: #ccc;
            border-radius: 4px;
        }

        .equation-container::-webkit-scrollbar-track {
            background-color: #f0f0f0;
        }
    </style>
</head>

<body>
    <h1>Beta Target Encoding</h1>

    <p>Bayesian Target Encoding is a feature engineering technique used to map categorical variables into numeric
        variables. When a common conjugate prior is used, the Bayesian framework is particularly simple requiring only
        minimal updates as new data is acquired and is thus well-suited for online learning. Furthermore, the Bayesian
        approach can make choosing and interpreting hyperparameters intuitive. In this blog post, we'll look at the tast
        of encoding categorical variables for binary classification tasks. In this case, the target distribution can be
        modeled with a Bernoulli distribution which has the beta distribution as conjugate prior, hence the name beta
        target encoding. </p>

    <h2>Target Encoding</h2>

    <p>All ML algorithms ultimately require numeric input data. There are many techniques for converting a categorical
        variable (e.g., the color "red", "green", "blue") into a numeric variable. Target encoding and categorical
        embeddings two popular and powerful techniques for doing so.
        When done properly, target encoding can be very effective. Target encoding is built into popular ML algorithms
        such as <a
            href="http://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features">LightGBM</a>
        and <a
            href="https://tech.yandex.com/catboost/doc/dg/concepts/algorithm-main-stages_cat-to-numberic-docpage/">CatBoost</a>.
    </p>

    <p>In supervised learning, we are given \(N\) data points \((x_i, y_i)\) and tasked with learning a mapping from
        input variable \(x\) to the target variable \(y\). Suppose \(x\) is categorical and takes on one of \(l\)
        possible (non-numeric) levels \(x\in \left\{ x^{(1)}, x^{(2)}, \dots, x^{(l)} \right\}\). Target encoding maps
        each level of \(x\) into a feature \(\phi\in \left\{ \phi^{(1)}, \phi^{(2)}, \dots, \phi^{(l)} \right\}\) in the
        following way:</p>

    <div class="equation-container">
        $$
        \phi^{(j)} = \frac{1}{N^{(j)}} \sum_{i=1}^N y_i \cdot \mathrm{I} \! \left\{ x_i = x^{(j)} \right\}
        \tag{1}
        $$
    </div>

    <p>where \(N^{(j)} = \sum_{i=1}^N \mathrm{I} \! \left\{ x_i = x^{(j)} \right\}\) and \(\mathrm{I}\{\cdot\}\) is the
        indicator function. In other words, \(\phi^{(j)}\) is the average \(y\)-value in level \(j\) and \(N^{(j)}\) is
        the total number of observations in level \(j\).</p>

    <h3>Overfitting and Regularization</h3>

    <p>Target-encoded variables are inherently leaky; that is, their construction requires information that we will not
        have when we make predictions. To see why this can be a problem, consider the case when \(N^{(j)} = 1\). In this
        case, the summation in \((1)\) involves only one non-zero term when \(x_i = x^{(j)}\). Thus, we have \(\phi_i =
        y_i\); that is, \(\phi_i\) is encoded with exactly the value we are trying to predict. Using these
        target-encoded features as is will lead to overfitting of the training set and poor generalization of our
        models. This problem is acute when dealing with high cardinality categorical variables. We can combat the data
        leakage problem by smoothing the estimate of the average \(y\)-value in level \(j\). In particular,</p>

    <div class="equation-container">
        $$
        \phi^{(j)} = \lambda^{(j)} \cdot \frac{1}{N} \sum_{i=1}^N y_i + \left( 1 - \lambda^{(j)} \right) \cdot
        \frac{1}{N^{(j)}} \sum_{i=1}^N y_i \cdot \mathrm{I} \! \left\{ x_i = x^{(j)} \right\}, \tag{2}
        $$
    </div>

    <p>which is just a weighted average of the estimate in \((1)\) and the average \(y\)-value value over the entire
        training set. The \(\lambda^{(j)}\) is a hyperparameter that controls the amount of smoothing in the estimate.
        Alternatively, we could address data leakage by adding noise to the estimate</p>



    <div class="equation-container">
        $$
        \phi^{(j)} = \eta \cdot \epsilon^{(j)} + (1- \eta) \cdot \frac{1}{N^{(j)}} \sum_{i=1}^N y_i \cdot
        \mathrm{I} \!
        \left\{ x_i = x^{(j)} \right\}. \tag{3}
        $$
    </div>


    <p>The hyperparameters \(\lambda^{(j)}\), \(\eta\), and the distribution of the random variable \(\epsilon^{(j)}\)
        need to be chosen carefully for target encoding to perform well. By adopting a Bayesian framework, we can handle
        regularization in a systematic yet intuitive way. Furthermore, the Bayesian framework is well-suited for online
        learning requiring only minimal updates as new data is acquired. In the next section, we will discuss target
        encoding in the context of binary classification.</p>

    <h2>Bayesian Target Encoding</h2>

    <p>In Bayesian statistics, we represent our beliefs about the world using a prior distribution. Approaching target
        encoding from a Bayesian perspective has the following benefits:</p>

    <ul>
        <li>hyperparameters are easy to interpret</li>
        <li>hyperparameter estimation is well-suited for online learning</li>
        <li>generalizing the encoding to statistics other than the mean is easy</li>
        <li>choosing the distribution for the noise random variable is easy</li>
    </ul>

    <p>The analysis here considers target encoding for binary classification where the target variable \(y\) takes on
        two discrete values \(\{0,1\}\). The same line of reasoning applies to target encoding for soft binary
        classification where \(y\) takes on values in the interval \([0, 1]\). The general theory applies to other
        target variable types.</p>

    <h3>The Beta Distribution<sup><a href="https://en.wikipedia.org/wiki/Beta_distribution">1</a></sup></h3>

    <p>It is convenient to model binary target variables using the Bernoulli distribution, which has the Beta
        distribution as its conjugate prior. The Beta distribution is parameterized by \(\alpha\) and \(\beta\), which
        can respectively be thought of as the number of positive and negative examples in a repeated Binomial
        experiment. Many useful statistics of the Beta distribution can written in terms of \(\alpha\) and \(\beta\);
        for example, the mean</p>

    <div class="equation-container">
        $$
        \mu = \frac{\alpha}{\alpha + \beta}, \tag{4}
        $$
    </div>

    <p>and the skewness</p>

    <div class="equation-container">
        $$
        \gamma = \frac{2 (\beta - \alpha) \sqrt{\alpha + \beta + 1}}{(\alpha + \beta+1)\sqrt{\alpha\beta}}. \tag{6}
        $$
    </div>


    <h3>Interpreting the Hyperparameters<sup><a
                href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter9.pdf">2</a></sup>
    </h3>

    <p>When deciding on the prior distribution, we need to set values for \(\alpha_{prior}\) and \(\beta_{prior}\). One
        way to do this is with grid search and cross-validation. However, we should not want to search over
        \(\alpha_{prior}\) and \(\beta_{prior}\) independently. Instead, we can reduce the search space by requiring
        \(\alpha\) and \(\beta\) to be such that the mean of the prior distribution is constant. The obvious choice for
        prior mean is the average \(y\)-value in the training set \(\mu_{prior} = \frac{1}{N} \sum_{i=1}^N y_i\), but
        the prior mean can also be set in other ways. If the categorical variables are hierarchical, then the prior mean
        can come from one step up in the hierarchy. With the mean known, we can reparameterize the beta distribution in
        terms of \(\tau = \alpha_{prior} + \beta_{prior}\). Intuitively, \(\tau\) can be thought of the effective sample
        size from the prior distribution. Rewriting \((4)\) in terms of \(\tau\) we get</p>

    <div class="equation-container">
        $$
        \alpha_{prior} = \tau \cdot \mu_{prior} \quad\text{and}\quad \beta_{prior} = \tau \cdot(1 - \mu_{prior}).
        $$
    </div>

    <p>The parameters of the posterior distribution are given by</p>

    <div class="equation-container">
        $$
        \alpha_{posterior}^{(j)} = \alpha_{prior}+ \sum_{i=1}^N y_i \cdot \mathrm{I}\! \left\{x=x^{(j)}\right\}
        $$
    </div>

    <p>and</p>

    <div class="equation-container">
        $$
        \beta_{posterior}^{(j)} = \beta_{prior} + N^{(j)} - \alpha_{posterior}^{(j)}.
        $$
    </div>

    <p>These equations show the simple update rules for the posterior parameters---all we need to keep track of is the
        sum of the \(y\)-values in a level and the total number of observations in that level. When the posterior
        parameters are known, we can use them to calculate other statistics, such as those in \((4)\), \((5)\), and
        \((6)\). To see how \(\tau\) controls complexity let's rewrite the posterior mean as</p>

    <div class="equation-container">
        $$
        \mu_{posterior}^{(j)} = \lambda^{(j)} \cdot \mu_{prior} + \left( 1-\lambda^{(j)} \right) \cdot \frac{1}{N^{(j)}}
        \sum_{i=1}^N y_i \cdot \mathrm{I}\! \left\{x=x^{(j)}\right\},
        $$
    </div>

    <p>where \(\lambda^{(j)} = \tau \,\big/\!\left( N^{(j)}+\tau \right).\) Notice that this equation has the same
        functional form as \((2)\). We see that increasing \(\tau\) brings the estimate closer to the prior mean and
        that decreasing \(\tau\) brings it closer to the in-level sample average.</p>

    <p>Recall that when we use additive noise for regularization \((3)\), we need to specify the distribution of the
        noise random variable \(\epsilon^{(j)}\). This is natural to do when taking the Bayesian approach. In
        particular, we can set \(\epsilon^{(j)} \sim \mathrm{Beta}\!\left(\alpha_{posterior}^{(j)},
        \beta_{posterior}^{(j)} \right)\). The only hyperparameter left to tune is the noise strength \(\eta\). Notice
        that from \((5)\), the variance of the noise distribution will shrink as we obtain more observations. What this
        means is that as the number of observations increases, the noise distribution will converge to a delta function
        centered at \(\mu_{posterior}^{(j)}\). Thus, we have the nice property that the amount of regularization
        decreases as the number of observations increases.</p>

    <!-- <h2>Case Study: Kaggle - Avito Demand Prediction<sup><a
                href="https://www.kaggle.com/mmotoki/avito-target-encoding">3</a></sup></h2>

    <p>My team and I used Bayesian target encoding in the recent Kaggle competition <a
            href="https://www.kaggle.com/c/avito-demand-prediction">Avito Demand Prediction Challenge</a> where we
        placed 14th out of 1,917 teams. For a more detailed write-up about our team's solution, see <a
            href="https://www.kaggle.com/c/avito-demand-prediction/discussion/60059">Peter Hurford's post</a>. The task
        was to predict demand for an online advertisement based on its full description (title, description, images,
        etc.), its context (geographically where it was posted, similar ads already posted), and historical demand for
        similar ads in similar contexts. The competition metric was the root mean squared error between our prediction
        and the probability of a deal. We found that the Bayesian target-encoded features outperformed the built-in
        LightGBM categorical variable encoding. The graph below shows the performance of a LightGBM model using target
        encoding for various values of \(\tau\) relative to a baseline LightGBM model using built-in categorical
        encoding. The best model uses Bayesian target-encoded features with a hyperparameter setting of \(\tau=10\).</p>


    <p class="caption">The figure above shows the relative improvement of the beta target-encoded features compared to
        the built-in LightGBM encodings. We see that the beta target encodings are better than the baseline for a wide
        range of hyperparameter settings.</p> -->

    <hr>

    <h3>References</h3>

    <ol>
        <li>A more comprehensive list of statistics for the Beta distribution.</li>
        <li>Information about Bayesian statistics and conjugate priors.</li>
        <li>Code for implementing the Bayesian target encoding.</li>
    </ol>

</body>

</html>